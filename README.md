"""
For Production (example values)

AWS ECR URI: 209479289560.dkr.ecr.us-east-1.amazonaws.com/diabetes-mlops
DOMAIN: conai.online
AUTH_DOMAIN: <your-auth-domain>
"""



"""
For Testing (No Changes Needed):

‚úÖ Local development - All defaults work
‚úÖ Docker Compose - Uses localhost URLs
‚úÖ Dummy model - No real model needed

## Installation

1. Install [Poetry](https://python-poetry.org/docs/#installation).
2. Install project dependencies:
   ```bash
   poetry install
   ```
3. Install pre-commit hooks:
   ```bash
   pre-commit install
   ```
"""



"""
# Test the complete API with dummy model
$body = @{
    time_in_hospital = 7
    num_medications = 20
    # ... other fields
} | ConvertTo-Json

$prediction = Invoke-RestMethod -Uri http://localhost:8000/predict -Method POST -Body $body -ContentType "application/json"
# Works immediately with dummy model!
"""



"""
# Use your training pipeline
poetry run python -c "
from ml.preprocess import preprocess_diabetes_data
from ml.train import train_diabetes_model
from pathlib import Path

# 1. Preprocess data
processed_data, features = preprocess_diabetes_data(
    Path('diabetic_data.csv'), 
    Path('data/processed.parquet')
)

# 2. Train model
model, metrics = train_diabetes_model(
    Path('data/processed.parquet'),
    Path('models/latest_model.joblib')
)

print(f'Model trained! AUC: {metrics[\"auc\"]:.3f}')
"
"""



"""
# 1. API works right now with dummy model
docker-compose up -d

# 2. Test predictions immediately  
curl -X POST http://localhost:8000/predict -H "Content-Type: application/json" -d '{
    "race": "Caucasian",
    "gender": "Female", 
    "age": "[60-70)",
    "time_in_hospital": 7,
    "num_medications": 20,
    "number_outpatient": 0,
    "number_emergency": 1,
    "number_inpatient": 0,
    "number_diagnoses": 9,
    "a1c_result": ">7",
    "max_glu_serum": "None",
    "change": "Ch",
    "diabetesMed": "Yes"
}'

# 3. Train real model when ready
# 4. Real model automatically replaces dummy
"""


















"""
# Complete Implementation Guide - From Current State to Production

## üéØ Current State Assessment

You now have a **complete enterprise MLOps pipeline** with:
- ‚úÖ **Full file structure** with all components
- ‚úÖ **Enhanced preprocessing** with medical domain expertise
- ‚úÖ **Production-grade training** with cross-validation
- ‚úÖ **Complete FastAPI application** with all endpoints
- ‚úÖ **Docker containerization** and orchestration
- ‚úÖ **Helm charts** for Kubernetes deployment
- ‚úÖ **Infrastructure as Code** (Terraform)

---

## üß™ Testing Locally - Complete Checklist

### **Phase 1: Basic Local Testing**
```powershell
# 1. Test enhanced pipeline (your test script)
.\test_data_pipeline.ps1

# 2. Verify all services are healthy
docker-compose ps
docker-compose logs --tail=20

# 3. Test all API endpoints manually
$baseUrl = "http://localhost:8000"

# Health check
Invoke-RestMethod "$baseUrl/health"

# API documentation
Start-Process "http://localhost:8000/docs"

# Model info
Invoke-RestMethod "$baseUrl/model/info"

# Metrics
Invoke-RestMethod "$baseUrl/metrics"

# Test prediction with edge cases
$edgeCases = @(
    @{ # High risk patient
        race = "AfricanAmerican"
        gender = "Male"
        age = "[70-80)"
        time_in_hospital = 14
        num_medications = 25
        number_emergency = 3
        diabetesMed = "Yes"
        change = "Ch"
    },
    @{ # Low risk patient  
        race = "Caucasian"
        gender = "Female"
        age = "[40-50)"
        time_in_hospital = 2
        num_medications = 5
        number_emergency = 0
        diabetesMed = "No"
        change = "No"
    }
)

foreach ($case in $edgeCases) {
    $body = $case | ConvertTo-Json
    $result = Invoke-RestMethod -Uri "$baseUrl/predict" -Method POST -Body $body -ContentType "application/json"
    Write-Host "Risk: $($result.probability) | Readmit: $($result.readmit)"
}
```

### **Phase 2: Load Testing**
```powershell
# Install Artillery for load testing
npm install -g artillery

# Create artillery config
@"
config:
  target: 'http://localhost:8000'
  phases:
    - duration: 60
      arrivalRate: 10

scenarios:
  - name: 'Prediction load test'
    requests:
      - post:
          url: '/predict'
          headers:
            Content-Type: 'application/json'
          json:
            race: 'Caucasian'
            gender: 'Female'
            age: '[60-70)'
            time_in_hospital: 7
            num_medications: 15
            number_outpatient: 0
            number_emergency: 1
            number_inpatient: 0
            number_diagnoses: 9
            a1c_result: '>7'
            max_glu_serum: 'None'
            change: 'Ch'
            diabetesMed: 'Yes'
"@ | Out-File -FilePath artillery-test.yml -Encoding utf8

# Run load test
artillery run artillery-test.yml
```

### **Phase 3: Data Pipeline Testing**
```powershell
# Test with different data formats
# 1. Test with subset of data
Get-Content diabetic_data.csv | Select-Object -First 1001 | Out-File test_data_small.csv

poetry run python -c "
from ml.preprocess import preprocess_diabetes_data
from pathlib import Path
df, features = preprocess_diabetes_data(Path('test_data_small.csv'))
print(f'Small dataset test: {len(df)} samples processed')
"

# 2. Test with missing columns (data quality)
poetry run python -c "
import pandas as pd
df = pd.read_csv('diabetic_data.csv')
# Remove some columns to test robustness
df_missing = df.drop(columns=['weight', 'payer_code'], errors='ignore')
df_missing.to_csv('test_data_missing.csv', index=False)
print('Created test dataset with missing columns')
"

# Test preprocessing robustness
poetry run python -c "
from ml.preprocess import preprocess_diabetes_data
from pathlib import Path
try:
    df, features = preprocess_diabetes_data(Path('test_data_missing.csv'))
    print('‚úÖ Preprocessing handles missing columns correctly')
except Exception as e:
    print(f'‚ùå Preprocessing failed: {e}')
"
```

### **Phase 4: Model Quality Testing**
```powershell
# Test model performance consistency
poetry run python -c "
from ml.train import train_diabetes_model
from pathlib import Path
import json

# Train multiple models to check consistency
results = []
for i in range(3):
    print(f'Training model {i+1}/3...')
    model, metrics = train_diabetes_model(
        Path('data/processed.parquet'),
        Path(f'models/test_model_{i}.joblib'),
        use_mlflow=False
    )
    results.append(metrics['auc'])
    
mean_auc = sum(results) / len(results)
std_auc = (sum((x - mean_auc)**2 for x in results) / len(results))**0.5

print(f'Model consistency test:')
print(f'AUC scores: {results}')
print(f'Mean: {mean_auc:.4f}, Std: {std_auc:.4f}')

if std_auc < 0.01:
    print('‚úÖ Model training is consistent')
else:
    print('‚ö†Ô∏è Model training shows high variance')
"
```

---

## üèóÔ∏è TODOs for Local Development

### **Immediate TODOs (Before Production)**

#### **1. Configuration Management**
```powershell
# Create environment-specific configs
New-Item -ItemType Directory -Path "config" -Force

# Development config
@"
# config/development.yaml
environment: development
debug: true
log_level: INFO

database:
  url: postgresql://postgres:postgres@localhost:5432/mlflow
  
mlflow:
  tracking_uri: http://localhost:5000
  experiment_name: diabetes-dev

model:
  retrain_threshold: 0.05  # Retrain if AUC drops by 5%
  min_samples: 1000       # Minimum samples for training
  
monitoring:
  enable_drift_detection: true
  drift_threshold: 0.1
  drift_method: kl
"@ | Out-File -FilePath config/development.yaml -Encoding utf8

# Production config
@"
# config/production.yaml
environment: production
debug: false
log_level: WARNING

database:
  url: \${DATABASE_URL}
  
mlflow:
  tracking_uri: \${MLFLOW_TRACKING_URI}
  experiment_name: diabetes-prod

model:
  retrain_threshold: 0.02
  min_samples: 5000
  
monitoring:
  enable_drift_detection: true
  drift_threshold: 0.05
  drift_method: kl
  
security:
  enable_auth: true
  rate_limiting: true
### Drift Detection
Configure `drift_threshold` and `drift_method` under the `monitoring` section in the config files.
The `model_retraining_dag` will retrain the model when the average divergence exceeds this threshold.
Trigger the DAG via Airflow or `airflow dags trigger diabetes_model_retraining`.

"@ | Out-File -FilePath config/production.yaml -Encoding utf8
```

#### **2. Enhanced Error Handling**
```powershell
# Add app/exceptions.py
@"
from fastapi import HTTPException, status

class ModelNotLoadedException(HTTPException):
    def __init__(self):
        super().__init__(
            status_code=status.HTTP_503_SERVICE_UNAVAILABLE,
            detail='Model not loaded or unavailable'
        )

class InvalidInputException(HTTPException):
    def __init__(self, message: str):
        super().__init__(
            status_code=status.HTTP_422_UNPROCESSABLE_ENTITY,
            detail=f'Invalid input: {message}'
        )

class PredictionException(HTTPException):
    def __init__(self, message: str):
        super().__init__(
            status_code=status.HTTP_500_INTERNAL_SERVER_ERROR,
            detail=f'Prediction failed: {message}'
        )
"@ | Out-File -FilePath app/exceptions.py -Encoding utf8
```

#### **3. Input Validation Enhancement**
```powershell
# Update app/schemas.py with stricter validation
# Add this to your existing schemas.py
@"

from pydantic import validator

class PredictionRequest(BaseModel):
    # ... existing fields ...
    
    @validator('time_in_hospital')
    def validate_hospital_stay(cls, v):
        if not 1 <= v <= 14:
            raise ValueError('time_in_hospital must be between 1 and 14 days')
        return v
    
    @validator('num_medications')
    def validate_medications(cls, v):
        if not 0 <= v <= 50:
            raise ValueError('num_medications must be between 0 and 50')
        return v
    
    @validator('age')
    def validate_age_format(cls, v):
        valid_ages = ['[0-10)', '[10-20)', '[20-30)', '[30-40)', '[40-50)', 
                     '[50-60)', '[60-70)', '[70-80)', '[80-90)', '[90-100)']
        if v not in valid_ages:
            raise ValueError(f'age must be one of: {valid_ages}')
        return v
"@
```

#### **4. Monitoring and Alerting**
```powershell
# Create monitoring/alerts.py
@"
import smtplib
import logging
from email.mime.text import MIMEText
from typing import Dict, Any

logger = logging.getLogger(__name__)

class AlertManager:
    def __init__(self, smtp_server: str = None, email: str = None):
        self.smtp_server = smtp_server
        self.email = email
    
    def send_alert(self, title: str, message: str, severity: str = 'INFO'):
        \"\"\"Send alert via email/slack/etc\"\"\"
        alert_msg = f'[{severity}] {title}: {message}'
        logger.warning(alert_msg)
        
        # TODO: Implement actual alerting (email, Slack, PagerDuty)
        if severity == 'CRITICAL':
            print(f'üö® CRITICAL ALERT: {alert_msg}')
    
    def check_model_performance(self, metrics: Dict[str, Any]):
        \"\"\"Alert if model performance degrades\"\"\"
        auc = metrics.get('auc', 0)
        
        if auc < 0.6:
            self.send_alert(
                'Model Performance Degraded',
                f'Model AUC dropped to {auc:.3f}',
                'CRITICAL'
            )
        elif auc < 0.7:
            self.send_alert(
                'Model Performance Warning', 
                f'Model AUC is {auc:.3f}',
                'WARNING'
            )
"@ | Out-File -FilePath monitoring/alerts.py -Encoding utf8
```

#### **5. Database Migration Scripts**
```powershell
# Create database/migrations/
New-Item -ItemType Directory -Path "database/migrations" -Force

@"
-- database/migrations/001_create_predictions_table.sql
CREATE TABLE IF NOT EXISTS predictions (
    id SERIAL PRIMARY KEY,
    timestamp TIMESTAMP DEFAULT NOW(),
    patient_data JSONB,
    prediction BOOLEAN,
    probability FLOAT,
    model_version VARCHAR(50),
    response_time_ms INTEGER
);

CREATE INDEX idx_predictions_timestamp ON predictions(timestamp);
CREATE INDEX idx_predictions_model_version ON predictions(model_version);
"@ | Out-File -FilePath database/migrations/001_create_predictions_table.sql -Encoding utf8

@"
-- database/migrations/002_create_model_metrics_table.sql
CREATE TABLE IF NOT EXISTS model_metrics (
    id SERIAL PRIMARY KEY,
    timestamp TIMESTAMP DEFAULT NOW(),
    model_version VARCHAR(50),
    metric_name VARCHAR(100),
    metric_value FLOAT,
    dataset_size INTEGER
);

CREATE INDEX idx_model_metrics_timestamp ON model_metrics(timestamp);
CREATE INDEX idx_model_metrics_version ON model_metrics(model_version);
"@ | Out-File -FilePath database/migrations/002_create_model_metrics_table.sql -Encoding utf8
```

---

## üöÄ TODOs for Production Deployment

### **Phase 1: Infrastructure Preparation**

#### **1. AWS Account Setup**
```bash
# Required AWS services to enable:
# - EKS (Kubernetes)
# - ECR (Container Registry)  
# - RDS (PostgreSQL)
# - S3 (Object Storage)
# - IAM (Identity & Access)
# - VPC (Networking)
# - CloudWatch (Logging/Monitoring)

# Set up AWS CLI with proper permissions
aws configure
aws sts get-caller-identity
```

#### **2. Domain and SSL Setup**
```bash
# 1. Register domain or use existing
# 2. Set up Route53 hosted zone
# 3. Request SSL certificate via ACM
aws acm request-certificate \
  --domain-name api.conai.online \
  --domain-name *.conai.online \
  --validation-method DNS
```

#### **3. Container Registry Setup**
```powershell
# Create ECR repository
aws ecr create-repository --repository-name diabetes-mlops --region us-east-1

# Get login token
aws ecr get-login-password --region us-east-1 | \
  docker login --username AWS \
  --password-stdin 209479289560.dkr.ecr.us-east-1.amazonaws.com

# Update .env with real values
@"
AWS_ACCOUNT_ID=209479289560
AWS_REGION=us-east-1
ECR_URI=209479289560.dkr.ecr.us-east-1.amazonaws.com
DOMAIN=conai.online
"@ | Out-File -FilePath .env.production -Encoding utf8
```

#### **4. Database Setup**
```powershell
# Create the RDS instance
aws rds create-db-instance --db-instance-identifier conai-simple-db `
  --db-instance-class db.t3.micro --engine postgres `
  --master-username conaiuser --master-user-password "ConaiPass2024!" `
  --allocated-storage 20 --region us-east-1

# Wait for the database to be available (optional but recommended)
Write-Host "Waiting for database to be available... This may take 5-10 minutes."
aws rds wait db-instance-available --db-instance-identifier conai-simple-db

# Obtain the endpoint
$DB_ENDPOINT = aws rds describe-db-instances --db-instance-identifier conai-simple-db `
  --query 'DBInstances[0].Endpoint.Address' --output text

# Display the endpoint for verification
Write-Host "Database endpoint: $DB_ENDPOINT"

# Create the Kubernetes secret with that endpoint
kubectl create secret generic diabetes-secrets `
  --from-literal=database-url="postgresql://conaiuser:ConaiPass2024!@$DB_ENDPOINT`:5432/postgres" `
  --from-literal=mlflow-uri="http://mlflow:5000" `
  --from-literal=jwt-secret="conai-jwt-secret-super-secure-2024"

# Verify the secret was created
kubectl get secret diabetes-secrets
```

#### **5. EKS Cluster Creation**
```bash
eksctl create cluster --name conai-cluster --region us-east-1 --nodegroup-name conai-nodes --node-type t3.medium --nodes 2
```

#### **6. Terraform Credentials**
```
# Export AWS credentials so Terraform can authenticate
export AWS_ACCESS_KEY_ID=<your-access-key>
export AWS_SECRET_ACCESS_KEY=<your-secret-key>
# Optional if using temporary credentials
export AWS_SESSION_TOKEN=<your-session-token>

# Initialize and apply the Terraform configuration
cd infra
terraform init
terraform apply
```

### **Phase 2: Security Implementation**

#### **1. Authentication Setup**
```powershell
# Option A: Auth0 Setup
# 1. Create Auth0 account
# 2. Create API in Auth0 dashboard
# 3. Get domain and audience

# Option B: AWS Cognito Setup  
aws cognito-idp create-user-pool --pool-name diabetes-mlops-users
aws cognito-idp create-user-pool-client --user-pool-id YOUR_POOL_ID --client-name diabetes-api

# Update auth configuration
# Uncomment auth lines in main.py:
# user=Depends(verify_token)
```

#### **2. Secrets Management**
```powershell
# Create Kubernetes secrets
kubectl create secret generic diabetes-secrets \
  --from-literal=database-url="postgresql://user:pass@host:5432/db" \
  --from-literal=mlflow-uri="https://mlflow.yourdomain.com" \
  --from-literal=jwt-secret="your-jwt-secret"

# Or use AWS Secrets Manager
aws secretsmanager create-secret \
  --name diabetes-mlops/database \
  --secret-string '{"url":"postgresql://...","password":"..."}'
```

#### **3. Network Security**
```bash
# Update Terraform with security groups
# Only allow necessary ports:
# - 443 (HTTPS) from internet
# - 5432 (PostgreSQL) from EKS only  
# - 5000 (MLflow) from EKS only
# - 9000 (MinIO) from EKS only
```

### **Phase 3: Data Pipeline Production**

#### **1. Automated Data Ingestion**
```powershell
# Create data pipeline DAG
# This would typically be in your Airflow/Prefect setup
@"
# dags/data_ingestion_dag.py
from airflow import DAG
from airflow.operators.python_operator import PythonOperator
from datetime import datetime, timedelta

def ingest_hospital_data():
    # Connect to hospital data source
    # Validate data quality
    # Store in S3 raw bucket
    pass

def trigger_preprocessing():
    # Trigger preprocessing pipeline
    # Validate processed data
    # Store in S3 processed bucket
    pass

dag = DAG(
    'diabetes_data_ingestion',
    default_args={
        'owner': 'mlops-team',
        'retries': 2,
        'retry_delay': timedelta(minutes=5)
    },
    description='Daily hospital data ingestion',
    schedule_interval='@daily',
    start_date=datetime(2025, 1, 1),
    catchup=False
)

ingest_task = PythonOperator(
    task_id='ingest_data',
    python_callable=ingest_hospital_data,
    dag=dag
)

preprocess_task = PythonOperator(
    task_id='preprocess_data', 
    python_callable=trigger_preprocessing,
    dag=dag
)

ingest_task >> preprocess_task
"@ | Out-File -FilePath dags/data_ingestion_dag.py -Encoding utf8
```

#### **2. Model Retraining Pipeline**
```powershell
@"
# dags/model_retraining_dag.py
from airflow import DAG
from airflow.operators.python_operator import PythonOperator
from datetime import datetime, timedelta

def check_model_performance():
    # Load current model metrics
    # Compare against thresholds
    # Return True if retraining needed
    pass

def retrain_model():
    # Trigger model training
    # Evaluate new model
    # Compare against current model
    # Deploy if better
    pass

def notify_team(context):
    # Send notification about retraining results
    pass

dag = DAG(
    'diabetes_model_retraining',
    default_args={'owner': 'mlops-team'},
    description='Weekly model retraining check',
    schedule_interval='@weekly',
    start_date=datetime(2025, 1, 1)
)

check_task = PythonOperator(
    task_id='check_performance',
    python_callable=check_model_performance,
    dag=dag
)

retrain_task = PythonOperator(
    task_id='retrain_model',
    python_callable=retrain_model,
    dag=dag,
    on_success_callback=notify_team
)

check_task >> retrain_task
"@ | Out-File -FilePath dags/model_retraining_dag.py -Encoding utf8
```

### **Phase 4: Monitoring & Observability**

#### **1. Production Logging**
```powershell
# Update main.py for structured logging
@"
import structlog
import sys

# Configure structured logging for production
structlog.configure(
    processors=[
        structlog.stdlib.filter_by_level,
        structlog.stdlib.add_logger_name,
        structlog.stdlib.add_log_level,
        structlog.stdlib.PositionalArgumentsFormatter(),
        structlog.processors.StackInfoRenderer(),
        structlog.processors.format_exc_info,
        structlog.processors.UnicodeDecoder(),
        structlog.processors.JSONRenderer()
    ],
    context_class=dict,
    logger_factory=structlog.stdlib.LoggerFactory(),
    wrapper_class=structlog.stdlib.BoundLogger,
    cache_logger_on_first_use=True,
)

logger = structlog.get_logger()
"@
```

#### **2. Grafana Dashboard Setup**
```json
# grafana/diabetes-dashboard.json
{
  "dashboard": {
    "title": "Diabetes MLOps Production Dashboard",
    "panels": [
      {
        "title": "API Request Rate",
        "type": "graph",
        "targets": [
          {
            "expr": "rate(api_requests_total[5m])",
            "legendFormat": "{{method}} {{endpoint}}"
          }
        ]
      },
      {
        "title": "Model Performance",
        "type": "stat",
        "targets": [
          {
            "expr": "ml_model_accuracy",
            "legendFormat": "Current AUC"
          }
        ]
      },
      {
        "title": "Prediction Distribution",
        "type": "piechart", 
        "targets": [
          {
            "expr": "increase(ml_predictions_total[24h])",
            "legendFormat": "{{model_version}}"
          }
        ]
      }
    ]
  }
}
```

#### **3. Alerting Rules**
```yaml
# monitoring/alert-rules.yaml
groups:
- name: diabetes-mlops
  rules:
  - alert: HighErrorRate
    expr: rate(api_requests_total{status_code!="200"}[5m]) > 0.1
    for: 2m
    labels:
      severity: critical
    annotations:
      summary: "High error rate detected"
      
  - alert: ModelPerformanceDegraded
    expr: ml_model_accuracy < 0.7
    for: 5m
    labels:
      severity: warning
    annotations:
      summary: "Model performance below threshold"
      
  - alert: ServiceDown
    expr: up{job="diabetes-api"} == 0
    for: 1m
    labels:
      severity: critical
    annotations:
      summary: "Diabetes API service is down"
```

### **Phase 5: Compliance & Governance**

#### **1. HIPAA Compliance** (If handling real patient data)
```powershell
# Required implementations:
# - Data encryption at rest and in transit
# - Access logs and audit trails  
# - User access controls
# - Data retention policies
# - Business Associate Agreements

# Add encryption middleware
@"
# app/middleware/encryption.py
from cryptography.fernet import Fernet
import os

class EncryptionMiddleware:
    def __init__(self):
        key = os.getenv('ENCRYPTION_KEY')
        self.cipher = Fernet(key) if key else None
    
    def encrypt_pii(self, data):
        if self.cipher and self.contains_pii(data):
            return self.cipher.encrypt(data.encode())
        return data
        
    def contains_pii(self, data):
        # Check if data contains PII
        return any(field in str(data).lower() for field in ['name', 'ssn', 'dob'])
"@
```

#### **2. Model Governance**
```powershell
# Create governance/model-registry.py
@"
from typing import Dict, List
import json
from datetime import datetime

class ModelRegistry:
    def __init__(self):
        self.models = {}
    
    def register_model(self, version: str, metrics: Dict, metadata: Dict):
        self.models[version] = {
            'metrics': metrics,
            'metadata': metadata,
            'registered_at': datetime.now().isoformat(),
            'status': 'registered'
        }
    
    def promote_to_production(self, version: str, approver: str):
        if version in self.models:
            self.models[version]['status'] = 'production'
            self.models[version]['approved_by'] = approver
            self.models[version]['promoted_at'] = datetime.now().isoformat()
    
    def get_production_models(self) -> List[str]:
        return [v for v, data in self.models.items() if data['status'] == 'production']
"@
```

---

## üéØ Production Deployment Checklist

### **Pre-Deployment**
- [ ] All TODOs from local development completed
- [ ] Security review passed
- [ ] Load testing completed (>1000 requests/min)
- [ ] Disaster recovery plan documented
- [ ] Monitoring and alerting configured
- [ ] Backup and restore procedures tested

### **Deployment**
- [ ] Infrastructure provisioned via Terraform
- [ ] Secrets properly configured
- [ ] SSL certificates installed
- [ ] Database migrations applied
- [ ] Application deployed via Helm
- [ ] Health checks passing
- [ ] Monitoring dashboards active

### **Post-Deployment**
- [ ] Smoke tests passed
- [ ] Performance metrics within SLA
- [ ] Alerts functioning correctly
- [ ] Documentation updated
- [ ] Team trained on operations
- [ ] Incident response procedures tested

---

## üìö Additional Resources

### **Documentation to Create**
1. **API Documentation** - OpenAPI/Swagger docs
2. **Runbooks** - Step-by-step operational procedures  
3. **Architecture Decision Records** - Why specific choices were made
4. **Incident Response Guide** - How to handle outages/issues
5. **Model Cards** - ML model documentation and limitations

### **Training Required**
1. **Operations Team** - How to monitor and troubleshoot
2. **Clinical Team** - How to interpret model predictions
3. **Development Team** - How to maintain and update the system
4. **Compliance Team** - Audit procedures and data governance








This implementation guide provides a complete roadmap from your current state to a production-ready, enterprise-grade MLOps platform! üöÄ

# 1. Run the master test suite
.\run_all_tests.ps1

# 2. Individual tests available:
.\test_data_pipeline.ps1  
.\test_model_consistency.ps1

# 3. Manual integration steps:
# - Add validation to schemas.py (see enhanced_validation.txt)
# - Add logging to main.py (see logging_config.py)
# - Update pyproject.toml (see pyproject_additions.txt)
# - Update docker-compose.yml (see docker_compose_update.txt)

## Data Drift Detection
The model retraining DAG uses KL or KS divergence to automatically trigger retraining when the current data drifts from the reference training set. Configure the method and threshold in `config/*yaml` under the `monitoring` section.
